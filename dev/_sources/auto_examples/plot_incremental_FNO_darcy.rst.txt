
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_incremental_FNO_darcy.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_incremental_FNO_darcy.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_incremental_FNO_darcy.py:


Training a neural operator on Darcy-Flow - Author Robert Joseph George
========================================
In this example, we demonstrate how to use the small Darcy-Flow example we ship with the package on Incremental FNO and Incremental Resolution

.. GENERATED FROM PYTHON SOURCE LINES 9-21

.. code-block:: Python


    import torch
    import matplotlib.pyplot as plt
    import sys
    from neuralop.models import FNO
    from neuralop.data.datasets import load_darcy_flow_small
    from neuralop.utils import count_model_params
    from neuralop.training.incremental import IncrementalFNOTrainer
    from neuralop.data.transforms.data_processors import IncrementalDataProcessor
    from neuralop import LpLoss, H1Loss









.. GENERATED FROM PYTHON SOURCE LINES 22-23

Loading the Darcy flow dataset

.. GENERATED FROM PYTHON SOURCE LINES 23-31

.. code-block:: Python

    train_loader, test_loaders, output_encoder = load_darcy_flow_small(
        n_train=100,
        batch_size=16,
        test_resolutions=[16, 32],
        n_tests=[100, 50],
        test_batch_sizes=[32, 32],
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/neuraloperator/neuraloperator/neuralop/data/datasets/pt_dataset.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
      data = torch.load(
    Loading test db for resolution 16 with 100 samples 
    /home/runner/work/neuraloperator/neuraloperator/neuralop/data/datasets/pt_dataset.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
      data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())
    Loading test db for resolution 32 with 50 samples 




.. GENERATED FROM PYTHON SOURCE LINES 32-33

Choose device

.. GENERATED FROM PYTHON SOURCE LINES 33-35

.. code-block:: Python

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")








.. GENERATED FROM PYTHON SOURCE LINES 36-39

Set up the incremental FNO model
We start with 2 modes in each dimension
We choose to update the modes by the incremental gradient explained algorithm

.. GENERATED FROM PYTHON SOURCE LINES 39-45

.. code-block:: Python

    incremental = True
    if incremental:
        starting_modes = (2, 2)
    else:
        starting_modes = (16, 16)








.. GENERATED FROM PYTHON SOURCE LINES 46-47

set up model

.. GENERATED FROM PYTHON SOURCE LINES 47-57

.. code-block:: Python

    model = FNO(
        max_n_modes=(16, 16),
        n_modes=starting_modes,
        hidden_channels=32,
        in_channels=1,
        out_channels=1,
    )
    model = model.to(device)
    n_params = count_model_params(model)








.. GENERATED FROM PYTHON SOURCE LINES 58-59

Set up the optimizer and scheduler

.. GENERATED FROM PYTHON SOURCE LINES 59-85

.. code-block:: Python

    optimizer = torch.optim.Adam(model.parameters(), lr=8e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)


    # If one wants to use Incremental Resolution, one should use the IncrementalDataProcessor - When passed to the trainer, the trainer will automatically update the resolution
    # Incremental_resolution : bool, default is False
    #    if True, increase the resolution of the input incrementally
    #    uses the incremental_res_gap parameter
    #    uses the subsampling_rates parameter - a list of resolutions to use
    #    uses the dataset_indices parameter - a list of indices of the dataset to slice to regularize the input resolution
    #    uses the dataset_resolution parameter - the resolution of the input
    #    uses the epoch_gap parameter - the number of epochs to wait before increasing the resolution
    #    uses the verbose parameter - if True, print the resolution and the number of modes
    data_transform = IncrementalDataProcessor(
        in_normalizer=None,
        out_normalizer=None,
        positional_encoding=None,
        device=device,
        subsampling_rates=[2, 1],
        dataset_resolution=16,
        dataset_indices=[2, 3],
        epoch_gap=10,
        verbose=True,
    )

    data_transform = data_transform.to(device)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Original Incre Res: change index to 0
    Original Incre Res: change sub to 2
    Original Incre Res: change res to 8




.. GENERATED FROM PYTHON SOURCE LINES 86-87

Set up the losses

.. GENERATED FROM PYTHON SOURCE LINES 87-100

.. code-block:: Python

    l2loss = LpLoss(d=2, p=2)
    h1loss = H1Loss(d=2)
    train_loss = h1loss
    eval_losses = {"h1": h1loss, "l2": l2loss}
    print("\n### N PARAMS ###\n", n_params)
    print("\n### OPTIMIZER ###\n", optimizer)
    print("\n### SCHEDULER ###\n", scheduler)
    print("\n### LOSSES ###")
    print("\n### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###")
    print(f"\n * Train: {train_loss}")
    print(f"\n * Test: {eval_losses}")
    sys.stdout.flush()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ### N PARAMS ###
     2118817

    ### OPTIMIZER ###
     Adam (
    Parameter Group 0
        amsgrad: False
        betas: (0.9, 0.999)
        capturable: False
        differentiable: False
        eps: 1e-08
        foreach: None
        fused: None
        initial_lr: 0.008
        lr: 0.008
        maximize: False
        weight_decay: 0.0001
    )

    ### SCHEDULER ###
     <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f9ce9fd85e0>

    ### LOSSES ###

    ### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###

     * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f9cf42b6eb0>

     * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f9cf42b6eb0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f9cf42b6130>}




.. GENERATED FROM PYTHON SOURCE LINES 101-116

Set up the IncrementalTrainer
other options include setting incremental_loss_gap = True
If one wants to use incremental resolution set it to True
In this example we only update the modes and not the resolution
When using the incremental resolution one should keep in mind that the numnber of modes initially set should be strictly less than the resolution
Again these are the various paramaters for the various incremental settings
incremental_grad : bool, default is False
   if True, use the base incremental algorithm which is based on gradient variance
   uses the incremental_grad_eps parameter - set the threshold for gradient variance
   uses the incremental_buffer paramater - sets the number of buffer modes to calculate the gradient variance
   uses the incremental_max_iter parameter - sets the initial number of iterations
   uses the incremental_grad_max_iter parameter - sets the maximum number of iterations to accumulate the gradients
incremental_loss_gap : bool, default is False
   if True, use the incremental algorithm based on loss gap
   uses the incremental_loss_eps parameter

.. GENERATED FROM PYTHON SOURCE LINES 116-134

.. code-block:: Python



    # Finally pass all of these to the Trainer
    trainer = IncrementalFNOTrainer(
        model=model,
        n_epochs=20,
        data_processor=data_transform,
        device=device,
        verbose=True,
        incremental_loss_gap=False,
        incremental_grad=True,
        incremental_grad_eps=0.9999,
        incremental_loss_eps = 0.001,
        incremental_buffer=5,
        incremental_max_iter=1,
        incremental_grad_max_iter=2,
    )








.. GENERATED FROM PYTHON SOURCE LINES 135-136

Train the model

.. GENERATED FROM PYTHON SOURCE LINES 136-146

.. code-block:: Python

    trainer.train(
        train_loader,
        test_loaders,
        optimizer,
        scheduler,
        regularizer=False,
        training_loss=train_loss,
        eval_losses=eval_losses,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training on 100 samples
    Testing on [50, 50] samples         on resolutions [16, 32].
    Raw outputs of shape torch.Size([16, 1, 8, 8])
    [0] time=0.14, avg_loss=0.8267, train_err=11.8096
    Eval: 16_h1=0.8102, 16_l2=0.6643, 32_h1=0.8250, 32_l2=0.6596
    [1] time=0.13, avg_loss=0.7575, train_err=10.8212
    Eval: 16_h1=0.7362, 16_l2=0.5747, 32_h1=0.7570, 32_l2=0.5641
    [2] time=0.13, avg_loss=0.7239, train_err=10.3411
    Eval: 16_h1=0.7366, 16_l2=0.5766, 32_h1=0.7783, 32_l2=0.5661
    [3] time=0.13, avg_loss=0.7228, train_err=10.3255
    Eval: 16_h1=0.7465, 16_l2=0.5743, 32_h1=0.8169, 32_l2=0.5600
    [4] time=0.13, avg_loss=0.7274, train_err=10.3921
    Eval: 16_h1=0.7092, 16_l2=0.5530, 32_h1=0.7454, 32_l2=0.5419
    [5] time=0.13, avg_loss=0.7118, train_err=10.1691
    Eval: 16_h1=0.7132, 16_l2=0.5544, 32_h1=0.7537, 32_l2=0.5433
    [6] time=0.15, avg_loss=0.6916, train_err=9.8802
    Eval: 16_h1=0.7031, 16_l2=0.5461, 32_h1=0.7355, 32_l2=0.5360
    [7] time=0.14, avg_loss=0.6754, train_err=9.6491
    Eval: 16_h1=0.7058, 16_l2=0.5526, 32_h1=0.7365, 32_l2=0.5361
    [8] time=0.14, avg_loss=0.6691, train_err=9.5586
    Eval: 16_h1=0.6777, 16_l2=0.5305, 32_h1=0.7038, 32_l2=0.5220
    [9] time=0.15, avg_loss=0.6522, train_err=9.3177
    Eval: 16_h1=0.6972, 16_l2=0.5441, 32_h1=0.7197, 32_l2=0.5342
    Incre Res Update: change index to 1
    Incre Res Update: change sub to 1
    Incre Res Update: change res to 16
    [10] time=0.22, avg_loss=0.6335, train_err=9.0503
    Eval: 16_h1=0.6933, 16_l2=0.5498, 32_h1=0.7084, 32_l2=0.5363
    [11] time=0.21, avg_loss=0.6075, train_err=8.6785
    Eval: 16_h1=0.6825, 16_l2=0.5391, 32_h1=0.6986, 32_l2=0.5267
    [12] time=0.23, avg_loss=0.5724, train_err=8.1771
    Eval: 16_h1=0.6952, 16_l2=0.5516, 32_h1=0.7056, 32_l2=0.5335
    [13] time=0.23, avg_loss=0.5271, train_err=7.5303
    Eval: 16_h1=0.7562, 16_l2=0.5915, 32_h1=0.7667, 32_l2=0.5748
    [14] time=0.23, avg_loss=0.4876, train_err=6.9663
    Eval: 16_h1=0.7453, 16_l2=0.5880, 32_h1=0.7560, 32_l2=0.5700
    [15] time=0.24, avg_loss=0.4745, train_err=6.7789
    Eval: 16_h1=0.6888, 16_l2=0.5401, 32_h1=0.7067, 32_l2=0.5310
    [16] time=0.24, avg_loss=0.4645, train_err=6.6351
    Eval: 16_h1=0.7172, 16_l2=0.5724, 32_h1=0.7301, 32_l2=0.5611
    [17] time=0.24, avg_loss=0.4599, train_err=6.5699
    Eval: 16_h1=0.7613, 16_l2=0.5980, 32_h1=0.7752, 32_l2=0.5801
    [18] time=0.23, avg_loss=0.3874, train_err=5.5345
    Eval: 16_h1=0.6920, 16_l2=0.5396, 32_h1=0.7117, 32_l2=0.5285
    [19] time=0.25, avg_loss=0.3342, train_err=4.7749
    Eval: 16_h1=0.6870, 16_l2=0.5392, 32_h1=0.6975, 32_l2=0.5236

    {'train_err': 4.774891308375767, 'avg_loss': 0.3342423915863037, 'avg_lasso_loss': None, 'epoch_train_time': 0.2451173030000291, '16_h1': tensor(0.6870), '16_l2': tensor(0.5392), '32_h1': tensor(0.6975), '32_l2': tensor(0.5236)}



.. GENERATED FROM PYTHON SOURCE LINES 147-157

Plot the prediction, and compare with the ground-truth
Note that we trained on a very small resolution for
a very small number of epochs
In practice, we would train at larger resolution, on many more samples.

However, for practicity, we created a minimal example that
i) fits in just a few Mb of memory
ii) can be trained quickly on CPU

In practice we would train a Neural Operator on one or multiple GPUs

.. GENERATED FROM PYTHON SOURCE LINES 157-195

.. code-block:: Python


    test_samples = test_loaders[32].dataset

    fig = plt.figure(figsize=(7, 7))
    for index in range(3):
        data = test_samples[index]
        # Input x
        x = data["x"].to(device)
        # Ground-truth
        y = data["y"].to(device)
        # Model prediction
        out = model(x.unsqueeze(0))
        ax = fig.add_subplot(3, 3, index * 3 + 1)
        x = x.cpu().squeeze().detach().numpy()
        y = y.cpu().squeeze().detach().numpy()
        ax.imshow(x, cmap="gray")
        if index == 0:
            ax.set_title("Input x")
        plt.xticks([], [])
        plt.yticks([], [])

        ax = fig.add_subplot(3, 3, index * 3 + 2)
        ax.imshow(y.squeeze())
        if index == 0:
            ax.set_title("Ground-truth y")
        plt.xticks([], [])
        plt.yticks([], [])

        ax = fig.add_subplot(3, 3, index * 3 + 3)
        ax.imshow(out.cpu().squeeze().detach().numpy())
        if index == 0:
            ax.set_title("Model prediction")
        plt.xticks([], [])
        plt.yticks([], [])

    fig.suptitle("Inputs, ground-truth output and prediction.", y=0.98)
    plt.tight_layout()
    fig.show()



.. image-sg:: /auto_examples/images/sphx_glr_plot_incremental_FNO_darcy_001.png
   :alt: Inputs, ground-truth output and prediction., Input x, Ground-truth y, Model prediction
   :srcset: /auto_examples/images/sphx_glr_plot_incremental_FNO_darcy_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 6.212 seconds)


.. _sphx_glr_download_auto_examples_plot_incremental_FNO_darcy.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_incremental_FNO_darcy.ipynb <plot_incremental_FNO_darcy.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_incremental_FNO_darcy.py <plot_incremental_FNO_darcy.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_incremental_FNO_darcy.zip <plot_incremental_FNO_darcy.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
